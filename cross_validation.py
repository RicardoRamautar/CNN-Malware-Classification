import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from sklearn.model_selection import KFold, StratifiedKFold
from torch.utils.data import Subset
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, mean_absolute_error, accuracy_score
import seaborn as sns
import time
from torch.cuda.amp import GradScaler, autocast
import pandas as pd
from torchvision.transforms import InterpolationMode
from pathlib import Path

from PIL import Image
Image.MAX_IMAGE_PIXELS = None

from config import CV_PARAMS, INIT_CHANNELS, CV_DATASET_PATH, CV_TRANSFORM, NUM_EPOCHS
from CNN import CNN

if __name__ == '__main__':
    # Enable cuDNN autotuner for GPU accelerations
    torch.backends.cudnn.benchmark = True

    # Hyper-parameters 
    num_epochs = NUM_EPOCHS
    batch_size = 100
    learning_rate = 0.001

    dataset = ImageFolder(root=CV_DATASET_PATH, transform=CV_TRANSFORM)
    targets = dataset.targets

    # Set device on which PyTorch will perform tensor computations to gpu if possible
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = CNN(CV_PARAMS["nr_conv_layers"],
                CV_PARAMS["nr_ff_layers"],
                CV_PARAMS["out_channels"],
                CV_PARAMS["ff_output_sizes"],
                CV_PARAMS["kernel_size"], INIT_CHANNELS).to(device)

    # Set loss function to cross entropy loss
    criterion = nn.CrossEntropyLoss()

    # Parameters for K-Fold cross validation
    k_folds = 10
    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True)

    # Empty list to store true labels and predicted labels across all folds
    all_true_labels = []
    all_predicted_labels = []

    # Initialize gradscaler (AMP)
    scaler = GradScaler()

    all_instances = 0
    all_correct = 0

    # K-fold cross validation loop
    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, targets)):
        print(f'FOLD {fold}')
        print('--------------------------------')
        
        # Sample elements randomly to train and test set without replacement
        train_subsampler = Subset(dataset, train_ids)
        test_subsampler = Subset(dataset, test_ids)
        
        # Initialize data loaders for training and testing data
        train_loader = DataLoader(train_subsampler, batch_size=batch_size, 
                                shuffle=True, num_workers=2, pin_memory=True)
        test_loader = DataLoader(test_subsampler, batch_size=batch_size, 
                                shuffle=False, num_workers=2, pin_memory=True)
        
        # Create instance of CNN that is run on device (gpu)
        model = CNN(CV_PARAMS["nr_conv_layers"],
                    CV_PARAMS["nr_ff_layers"],
                    CV_PARAMS["out_channels"],
                    CV_PARAMS["ff_output_sizes"],
                    CV_PARAMS["kernel_size"], INIT_CHANNELS).to(device)
        
        # Initialize optimizer
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

        # Empty lists storing true and predicted labels for this fold
        fold_true_labels = []
        fold_predicted_labels = []
        learning_rates = []
        epochs = []
            
        # Run training loop for defined number of epochs
        for epoch in range(0, num_epochs):
            epoch_start_time = time.time()
            print(f'Starting epoch {epoch+1}')
            t_start = time.time()        
            
            # Set current loss value
            current_loss = 0.0
            
            # Iterate over dataloader for training
            for i, data in enumerate(train_loader, 0):
                # Get inputs and targets (ground truths)
                inputs, targets = data
                inputs = inputs.to(device)
                targets = targets.to(device)

                # Zero the gradients
                optimizer.zero_grad()
                            
                with autocast():
                    # Perform forward pass
                    outputs = model(inputs)

                    # Compute loss
                    loss = criterion(outputs, targets)
                
                # Perform backward pass
                scaler.scale(loss).backward()
        
                # Unscales gradients and calls or skips optimizer.step()
                scaler.step(optimizer)
                
                # Updates the scale for next iteration
                scaler.update()
                
                # Print statistics
                current_loss += loss.item()
                if i % 100 == 99:
                    print('Loss after mini-batch %5d: %.3f' %
                        (i + 1, current_loss / 100))
                    current_loss = 0.0
                    
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            learning_rates.append(current_lr)
            epochs.append(epoch + 1)

            epoch_end_time = time.time()
            print("Runtime epoch: ", epoch_end_time - epoch_start_time)

                    
        print('Training process has finished. Saving trained model.')
        print('Starting testing')

        # Evaluation for this fold
        correct, total = 0, 0
        with torch.no_grad():
            for images, labels in test_loader:
                images = images.to(device)
                labels = labels.to(device)
                images = images.to(device)
                outputs = model(images, apply_softmax = True)

                # Set total and correct
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                fold_true_labels.extend(labels.cpu().numpy())
                fold_predicted_labels.extend(predicted.cpu().numpy())

        all_true_labels.extend(fold_true_labels)
        all_predicted_labels.extend(fold_predicted_labels)
        
        val_accuracy = 100 * correct / total
        val_mae = mean_absolute_error(fold_true_labels, fold_predicted_labels)
        
        all_instances += total
        all_correct += correct

        # Print accuracy
        print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))
        print(f'MAE for fold {fold}: {val_mae}')
        print('--------------------------------')
        print('--------------------------------')
        
    print('Training completed.')
    conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)

    tot_accuracy = 100.0 * all_correct / all_instances
    tot_mae = mean_absolute_error(all_true_labels, all_predicted_labels)

    print(f'\nAccuracy over all folds: {tot_accuracy}')
    print(f'MAE over all folds: {tot_mae}')

    plt.figure(figsize=(10, 10))
    sns.heatmap(conf_matrix, annot=True, fmt='d', square=True, cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()

    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)

    precision, recall, f1_score, _ = precision_recall_fscore_support(all_true_labels, all_predicted_labels, average=None)
    metrics_table = pd.DataFrame({
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1_score
    })
    print(metrics_table)
